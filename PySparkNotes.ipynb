{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Q&A \n",
    "inspired by justmarkham/pandas-videos on github\n",
    "\n",
    "## Table of Contents\n",
    "+ [Installing and running a simple Spark environment]\n",
    "+ + [Developing on Apache Zeppelin]\n",
    "+ + [Developing on Jupyter]\n",
    "+ [What is pyspark?]\n",
    "\n",
    "\n",
    "\n",
    "### Datasets\n",
    "\n",
    "Filename | Description | Raw File | Original Source | Other\n",
    "--- | --- | --- | --- | ---\n",
    "[chipotle.tsv](data/chipotle.tsv) | Online orders from the Chipotle restaurant chain | [bit.ly/chiporders](http://bit.ly/chiporders) | [The Upshot](https://github.com/TheUpshot/chipotle) | [Upshot article](http://www.nytimes.com/interactive/2015/02/17/upshot/what-do-people-actually-order-at-chipotle.html)\n",
    "[drinks.csv](data/drinks.csv) | Alcohol consumption by country | [bit.ly/drinksbycountry](http://bit.ly/drinksbycountry) | [FiveThirtyEight](https://github.com/fivethirtyeight/data/tree/master/alcohol-consumption) | [FiveThirtyEight article](http://fivethirtyeight.com/datalab/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/)\n",
    "[imdb_1000.csv](data/imdb_1000.csv) | Top rated movies from IMDb | [bit.ly/imdbratings](http://bit.ly/imdbratings) | [IMDb](http://www.imdb.com/search/title?groups=top_1000&sort=user_rating&view=simple) | [Web scraping script](https://github.com/justmarkham/DAT5/blob/master/code/08_web_scraping.py)\n",
    "[titanic_test.csv](data/titanic_test.csv) | Testing set from Kaggle's Titanic competition | [bit.ly/kaggletest](http://bit.ly/kaggletest) | [Kaggle](https://www.kaggle.com/c/titanic) | [Data dictionary](https://www.kaggle.com/c/titanic/data)\n",
    "[titanic_train.csv](data/titanic_train.csv) | Training set from Kaggle's Titanic competition | [bit.ly/kaggletrain](http://bit.ly/kaggletrain) | [Kaggle](https://www.kaggle.com/c/titanic) | [Data dictionary](https://www.kaggle.com/c/titanic/data)\n",
    "[u.user](data/u.user) | Demographic information about MovieLens users | [bit.ly/movieusers](http://bit.ly/movieusers) | [GroupLens](http://grouplens.org/datasets/movielens/100k/) | [Data dictionary](http://files.grouplens.org/datasets/movielens/ml-100k-README.txt)\n",
    "[ufo.csv](data/ufo.csv) | Reports of UFO sightings from 1930-2000 | [bit.ly/uforeports](http://bit.ly/uforeports) | [National UFO Reporting Center](http://www.nuforc.org/webreports.html) | [Web scraping script](https://github.com/josiahdavis/josiahdavis.github.io/blob/master/supporting%20material/get_ufo_data.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0a. Install and running with Apache Zeppelin\n",
    "\n",
    "If you have already a Spark setup that you are using , we suggest to download and run zeppelin as described in \n",
    "https://zeppelin.apache.org/docs/0.7.3/install/install.html\n",
    "\n",
    "If you want to develop spark projects with relatively small data ( for limited resources) , we suggest you to use the Zeppelin docker images \n",
    "\n",
    "To persist logs and notebook directories, use the volume option for docker container.\n",
    "```bash\n",
    "docker run -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' --name zeppelin apache/zeppelin:0.7.3\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0a. Install and running with Jupyter and Apache Toree\n",
    "\n",
    "do the following steps\n",
    "\n",
    "1. Download and Install JDK\n",
    "\n",
    "```bash\n",
    "wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.rpm\n",
    "yum localinstall jdk-8u121-linux-x64.rpm\n",
    "```\n",
    "2. Download and Install Spark\n",
    "```bash\n",
    "wget -c https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
    "mkdir ~/spark\n",
    "tar -zxvf spark-2.2.0-bin-hadoop2.6.tgz -C ~/spark/\n",
    "```\n",
    "3. Set Environment variables\n",
    "\n",
    "Add following lines in your .bash_profile or .bashrc\n",
    "```bash\n",
    "export SPARK_HOME=~/spark/spark-2.2.0-bin-hadoop2.7\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "```\n",
    "4. Test Spark Installation\n",
    "\n",
    "You can start Spark shell and Python shell by typing below commands\n",
    "```bash\n",
    "#To start spark shell\n",
    "spark-shell\n",
    "#To start python shell\n",
    "pyspark\n",
    "```\n",
    "\n",
    "5. Download and Install Anaconda\n",
    "```bash\n",
    "wget -c https://repo.continuum.io/archive/Anaconda3-5.0.0.1-Linux-x86_64.sh\n",
    "bash Anaconda3-5.0.0.1-Linux-x86_64.sh\n",
    "```\n",
    "6. Download and Install Apache Toree\n",
    "```bash\n",
    "wget -c https://dist.apache.org/repos/dist/dev/incubator/toree/0.2.0/snapshots/dev1/toree-pip/toree-0.2.0.dev1.tar.gz\n",
    "pip install toree-0.2.0.dev1.tar.gz\n",
    "jupyter toree install --spark_home=$SPARK_HOME --interpreters=Scala,PySpark,SQL --user\n",
    "```\n",
    "7. Start Jupyter Server ( for local machine)\n",
    "```bash\n",
    "jupyter notebook --no-browser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is pyspark ?\n",
    "\n",
    "- [what is spark?](https://databricks.com/spark/about)\n",
    "- [spark main page](https://spark.apache.org/) \n",
    "- [pyspark documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "While using IDE the pyspark libraries should be included like\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "```\n",
    "While using jupyter notebook with the following command, sparkContext is already avaliable to use\n",
    "\n",
    "```bash\n",
    "MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"8G\" PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" ~/spark/bin/pyspark --master local[2]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit spark jobs to a cluster you must use spark-submit which is in the spark installation directory\n",
    "\n",
    "Simple submission\n",
    "```bash\n",
    "~/spark/bin/spark-submit examples/src/main/python/wordcount.py /etc/profile\n",
    "```\n",
    "\n",
    "Submission with parameters\n",
    "```bash\n",
    "export SPARK_MAJOR_VERSION=2\n",
    "PYSPARK_PYTHON=/usr/local/bin/python2.7 \\\n",
    "spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--num-executors 64 \\\n",
    "--driver-memory 10g \\\n",
    "--executor-memory 10g \\\n",
    "--executor-cores 4 \\\n",
    "--conf spark.yarn.maxAppAttempts=4 \\\n",
    "--conf spark.yarn.am.attemptFailuresValidityInterval=1h \\\n",
    "--conf spark.yarn.max.executor.failures=16 \\\n",
    "--conf spark.yarn.executor.failuresValidityInterval=1h \\\n",
    "--conf spark.task.maxFailures=8 \\\n",
    "--conf spark.speculation=true \\\n",
    "--conf spark.yarn.executor.memoryOverhead=6g \\\n",
    "--conf spark.driver.maxResultSize=6g \\\n",
    "--conf spark.executor.heartbeatInterval=20s \\\n",
    "--queue default \\\n",
    "examplecode.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How do I read a tabular data file into spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file should be on a file sytem (local or hdfs)\n",
    "filename='./data/chipotle.tsv'\n",
    "df = spark.read.csv(filename,header=True, inferSchema=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.cov?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df._sc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(order_id=1, quantity=1, item_name=u'Chips and Fresh Tomato Salsa', choice_description=u'NULL', item_price=u'$2.39 ')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+------------------+----------+\n",
      "|order_id|quantity|           item_name|choice_description|item_price|\n",
      "+--------+--------+--------------------+------------------+----------+\n",
      "|       1|       1|Chips and Fresh T...|              NULL|    $2.39 |\n",
      "|       1|       1|                Izze|      [Clementine]|    $3.39 |\n",
      "|       1|       1|    Nantucket Nectar|           [Apple]|    $3.39 |\n",
      "|       1|       1|Chips and Tomatil...|              NULL|    $2.39 |\n",
      "+--------+--------+--------------------+------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display in a nice format\n",
    "df.show(4,truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files without header\n",
    "userfilename = './data/u.user'\n",
    "userdf = spark.read.csv(userfilename,header=False, inferSchema=True,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------+-----+\n",
      "|_c0|_c1|_c2|          _c3|  _c4|\n",
      "+---+---+---+-------------+-----+\n",
      "|  1| 24|  M|   technician|85711|\n",
      "|  2| 53|  F|        other|94043|\n",
      "|  3| 23|  M|       writer|32067|\n",
      "|  4| 24|  M|   technician|43537|\n",
      "|  5| 33|  F|        other|15213|\n",
      "|  6| 42|  M|    executive|98101|\n",
      "|  7| 57|  M|administrator|91344|\n",
      "|  8| 36|  M|administrator|05201|\n",
      "|  9| 29|  M|      student|01002|\n",
      "| 10| 53|  M|       lawyer|90703|\n",
      "| 11| 39|  F|        other|30329|\n",
      "| 12| 28|  F|        other|06405|\n",
      "| 13| 47|  M|     educator|29206|\n",
      "| 14| 45|  M|    scientist|55106|\n",
      "| 15| 49|  F|     educator|97301|\n",
      "| 16| 21|  M|entertainment|10309|\n",
      "| 17| 30|  M|   programmer|06355|\n",
      "| 18| 35|  F|        other|37212|\n",
      "| 19| 40|  M|    librarian|02138|\n",
      "| 20| 42|  F|    homemaker|95660|\n",
      "+---+---+---+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to chage the headers\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+-----+\n",
      "|_c0|_c1|_c2|       _c3|  _c4|\n",
      "+---+---+---+----------+-----+\n",
      "|  1| 24|  M|technician|85711|\n",
      "|  2| 53|  F|     other|94043|\n",
      "|  3| 23|  M|    writer|32067|\n",
      "|  4| 24|  M|technician|43537|\n",
      "|  5| 33|  F|     other|15213|\n",
      "+---+---+---+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.3\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
